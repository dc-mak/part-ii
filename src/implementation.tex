% vim: set spell spelllang=en_gb tw=80:
\chapter{Implementation}\label{chap:impl}

% \guidance{%
%   This chapter should describe what was actually produced: the programs which
%   were written, the hardware which was built or the theory which was developed.
% 	Any design strategies that \textbf{looked ahead to the testing stage} might
% 	profitably be referred to (the professional approach again).\\
%   Descriptions of programs may include fragments of high-level code but large
%   chunks of code are usually best left to appendices or omitted altogether.
%   Analogous advice applies to circuit diagrams.\\ 
%   \textbf{Draw attention to the parts of the work which are not your own}. Making
%   effective use of powerful tools and pre-existing code is often laudable, and
%   will count to your credit if properly reported.\\
%   It should not be necessary to give a day-by-day account of the progress of the
%   work but \textbf{major milestones may sometimes be highlighted with advantage}.\\
% }

\prechapter{%
  In this chapter, I will describe how I implemented my project. What follows is
  an account of the programs I wrote, problems I encountered, solutions I
  implemented and tests I conducted using the project structure shown in
  Figure~\ref{fig:structure} as a guide.
}

\begin{figure}[t]
\centering
\input{img/structure.tex}
\caption{Structure of Project. Nodes show the format of the information. Edges
  show the transformations. Edges in green  show my contributions.}\label{fig:structure}
\end{figure}

\section{Coq object-files to CSV}

This section of implementation corresponds to ``dpdgraph++'' and ``\texttt{dpd2
csv}''on Figure~\ref{fig:structure}: modelling the data contained in and the
structure of a Coq library (of \texttt{.vo} compiled proof-scripts) as CSVs.
First, I will briefly describe the algorithm I used to construct a dependency
graph constructed from a compiled Coq library. Then, I will elaborate upon how I
modelled the Coq library by going through the attributes of each edge and node
in the dependency graph. Finally, I will describe the output format of the
\texttt{.dpd} file containing the graph constructed from the compiled Coq
library.

\subsection{Algorithm}

This is a high-level overview of the algorithm I used in ``dpdgraph++'' to
construct a dependency graph from a compiled Coq library.

\textbf{Input:} A Coq script containing a list of files in the library (I wrote
a shell-script to generate this file automatically) and the path to the compiled
Coq library.

\textbf{Output:} A \texttt{.dpd} file containing a description of a graph whose
nodes represent Coq proof-objects and whose edges represent various
relationships between the proof-objects.

\begin{enumerate}

  \item For each module, for each proof-object in the module:

    \begin{enumerate}
      \item if the proof-object is not in the set of nodes, add it.
      \item for each module in the chain of modules from the proof-object's parent
        to the root of the library: 
        \begin{enumerate}
          \item if the module is not in the set of nodes, add it.
          \item if there is not an edge between a module and its child in the
            set of edges, add it.
        \end{enumerate}
      \item collect the proof-object's dependencies by recursing over its AST.
      \item for each dependency: 
        \begin{enumerate} 
          \item if the dependency is not in the set of nodes, add it.
          \item if there is not an edge between the proof-object and its
            dependency in the set of edges, add it.
          \item otherwise, retrieve the edge and increment its \emph{weight
            attribute} (representing the number of uses)
        \end{enumerate}
    \end{enumerate}
  
  \item For each node in the set of nodes, output all of its attributes
    (attributes described in Subsection~\ref{subsec:modelling}; output format described in
    Subsection~\ref{subsec:translation}).

  \item For each edge in the set of edges, output all of its attributes (source,
    destination and weight; output format described in
    Subsection~\ref{subsec:translation}).

\end{enumerate}

\subsection{Modelling}\label{subsec:modelling}

Although I used dpdgraph as a starting point, I rewrote all of the
attribute-assigning code because the meaning of the attributes it initially
collected was not obvious and terminology it used did not correspond to
recognisable Coq constructs.

I added the following attributes and improved the dependency-collection for
inductive types and their constructors (as I will describe
in~\ref{subsubsec:typecon}~\nameref{subsubsec:typecon} on
page~\pageref{subsubsec:typecon}).

\subsubsection{Precise Kinds}\label{subsubsec:kinds}

A proof-object can be one of the following things, corresponding to an AST term:
\textsf{module}, \textsf{class}, \textsf{type\_constructor},
\textsf{inductive\_type}, \textsf{definition}, \textsf{assumption} or
\textsf{proof}.

Optionally, some terms have more precise terminology, for distinguishing
different constructs. For example, when writing Coq, there is no \texttt{Proof}
keyword; instead \texttt{Theorem}, \texttt{Lemma}, \texttt{Fact},
\texttt{Remark}, \texttt{Property}, \texttt{Proposition} and \texttt{Corollary}
all are \emph{synonyms} for proofs. Full details can be found in
Appendix~\ref{chapter:fullmodel}.

So to model this categorisation, I assigned each node a ``kind'' \emph{label}:
labels are strings used to group nodes into subsets; since a node can belong to
more than one subset, it can have more than one label assigned to it. To model
the more precise distinctions, I optionally assigned some nodes a ``subkind''
label.

\subsubsection{Recursive Modules}\label{subsubsec:recmodules}

Every proof-object is contained within a module and every module is either
contained in a file, is a file or is a directory. To model this inclusion
relationship, analyse module-level dependencies (like coqdep) and contrast the
results community detection algorithms with the module structure of a library, I
augmented ``dpdgraph++'' to include modules in the graph. Using the Coq API, I
could get the fully-qualified path (path from the library's root) of a
proof-object. Using a variant datatype, I expanded the type of a node in the
graph to include modules and propagated the changes throughout the project.

However, because the Coq API returned module paths as strings, modules were in
the model as a flat structure: modules could be related to objects but not to
other modules. I fixed this by inferring (splitting the fully-qualified path)
and adding all the ``ancestors'' of a module with the correct relationships
(parent as source, child as destination, repeatedly up to the root module).

\subsubsection{Types and Constructors}\label{subsubsec:typecon}

One of the most glaring omissions from dpdgraph's initial model was the
inability to relate a type to its constructor(s). To fix this,
I improved the dependency-collection code.

Expanding an AST term for type-constructors shows which type it constructed
(note that types have no information about which constructors construct them).
Since dependencies were constructed in a depth-first manner \emph{down} the AST,
I had to store the type and type-constructor relationship reversed but output it
in the correct order.

To do so, I compared each pair of nodes to see if it (a) was a type and a
constructor and if so, (b) the constructor's fully-qualified type matched the
fully-qualified name of the type. If both these criteria were met, I swapped the
direction of the edge output.

\subsubsection{Types}

% Conducting a \emph{cognitive walkthrough} (detailed in
% Chapter~\ref{chapter:evaluation}) spurred the addition of \emph{type
% signatures} to the model.

Another glaring omission from dpdgraph's initial model was the inability to see
the type of a proof-object. Type theory is central to a Coq user's work and
being able to include them in the model, would, along with kinds, subkinds and
modules, help towards meeting the modelling requirement~\ref{req:m1} of
including as much relevant data as possible. 

Coq's type-checking algorithms is complex. To replicate it within
``dpdgraph++'', I followed the functions called for the Coq command
\texttt{Check} \emph{<expression>} (for printing the type of a given
expression).  This led to the algorithm for \emph{getting} the type, which I
then implemented.

A subtlety I overcame was \emph{using} the output in \texttt{.dpd} and CSV
files. I replaced newlines, quotation marks, and commas with hash signs,
single-quote marks and underscores respectively, because the latter were used to
delimit data in \texttt{.dpd} and CSV files and would have otherwise caused
errors when they were being parsed into subsequent programs.

\subsubsection{Relationships}

Modelling relationships was the most interesting aspect of deciding how to
represent information. I wanted to keep consistent the notion of expanding a
node to see more details: if a user is looking at an object, they should be able
to expand the object to see what the object depends on; if a user is looking at
a module, they should be able to expand the module to see the objects contained
within that module; if a user is looking at a type, they should
be able to expand the type and see its constructors.

I modelled this idea with the following relationships:
\mintinline{cypher}{(src)-[:USES]->(dst)} for dependencies,
\mintinline{cypher}{(type)-[:CONSTRUCTED_BY]->(constr)} for (co-)inductive
types, and \mintinline{cypher}{(module)-[:CONTAINS]->(object)} for modules.

I had two problems whilst implementing these relationships. First was finding
and matching types and constructors (details of which are described
in~\ref{subsubsec:typecon}~\nameref{subsubsec:typecon} on
page~\pageref{subsubsec:typecon}).

Second was balancing expressiveness against simplicity. I considered
relationships of the following format, \texttt{X\_USES\_Y} for kinds X and Y.
Although this was useful for fewer kinds, I decided its specificity when
subkinds are included in the model made the model too large and complex (on the
order of $n^{2}$ relationships). Since Cypher allows pattern-matching and
filtering based on kinds and subkinds anyway, I chose the simplified model
presented above.

\subsection{Translation}\label{subsec:translation}

This subsection corresponds to ``\texttt{dpd2 csv}'' on
Figure~\ref{fig:structure}.  Once a model is constructed (in the form of a
graph) by ``dpdgraph++'' and output to a \texttt{.dpd} file, it is translated by
the \texttt{dpd2} tool to a CSV file for use by Neo4j's import tool to create a
database. I will now present an overview of the \texttt{.dpd} and CSV formats,
as well as the \texttt{dpd2} tool itself.

\subsubsection{\texttt{dpd} Format}

``dpdgraph++'' outputs the graph representing the model as a \texttt{.dpd} file
with the following format for nodes (one per line):

{\tt N: \emph{<id>} "\emph{<name>}" [\emph{<property>}=\emph{<value>}];}

for example (full type elided for brevity), 

{\tt N: 76 "matches" [type="... -> bool", subkind=fixpoint, kind=definition, path="RegExp.Definitions", ];}

and likewise for edges:

{\tt E: \emph{<src id>} \emph{<dst id>} [\emph{<property>}=\emph{<value>}];}

for example, 

{\tt E: 150 145 [type=CONTAINS, weight=1, ];}.

\subsubsection{CSV}\label{subsubsec:translationcsv}

Once it is output as a \texttt{.dpd} file, a model can be translated to various
other formats. For example, dpdgraph includes a tool to output a \texttt{.dot}
file (a format used extensively for \emph{visualising} graphs by many tools)
from a \texttt{.dpd} file. This is what was used to generate
Figure~\ref{fig:static}.

However, for the purpose of this project, I translated the \texttt{.dpd} file
(using the \texttt{dpd2} tool described below) to \emph{two} CSV files: one for
nodes and one for edges, for use with Neo4j's import tools with the following
headers.

\texttt{objectId:ID(Object), name, kind:LABEL, subkind:LABEL, path, type}

Here we see name, path and type declared as properties, kind and subkind
declared as labels and the objectId field declared as unique identifier (or in
relational terms, a key) for the nodes (in this schema, called ``Objects'') in
the graph.

\texttt{:START\_ID(Object), :END\_ID(Object), weight:int, :TYPE}

Similarly, here we see relationships (between ``Objects'' as declared
previously) named according to the value under the \texttt{:TYPE} column (e.g.
\texttt{CONTAINS}, \texttt{USES} or \texttt{CONSTRUCTED\_BY}), each with an
integer property, weight.

By using a CSV format, adding extra properties, labels and relationships becomes
very straightforward and allows for easy integration (with external tools) and
extension for new features.

\subsubsection{\texttt{dpd2} Tool}\label{subsubsec:dpd2}

Initially, this tool started out as the \texttt{dpd2dot} utility (bundled with
dpdgraph). I refactored it into a more general, \texttt{dpd2} tool which could
accept the file-type (\texttt{.dot} or CSV) as a command-line argument.

In using this tool, I discovered it had a bug. By default, \texttt{dpd2}
attempts to remove reflexive and transitive dependencies (i.e. $a \rightarrow a$
and removing $a \rightarrow c$ if $a \rightarrow b$ and $b \rightarrow c$) in a
depth-first manner.  For large graphs, doing so is stack-intensive, and thus
causes the aforementioned error.  Since this ``feature'' was unnecessary
(because it \emph{removed} useful information) and appeared time-consuming to
fix, I passed \texttt{-keep-trans} flag on subsequent uses to avoid the issue
altogether.

\section{CSV to Neo4j}

The ``import tool'' in Figure~\ref{fig:structure} is a command-line program
that is included with Neo4j. When given a target directory, a CSV file
containing the nodes of the graph and a separate CSV file containing the edges
of the graph, this import tool constructs a database. As explained
in~\ref{subsubsec:translationcsv}~\nameref{subsubsec:translationcsv} on
page~\pageref{subsubsec:translationcsv}, the headers determine the labels,
properties, IDs of nodes as well as the start- and end-points, properties and
type of edges.

\section{Query Library}

This section of the implementation corresponds to the ``Query Library''
(including the ``R Library'') shown in Figure~\ref{fig:structure}. I will now
outline the extensions I included to meet the interaction
(\ref{req:i1},~\ref{req:i2},~\ref{req:i3}) and computation
(\ref{req:c1},~\ref{req:c2}) requirements on page~\pageref{req:i1}.


\subsection{Java Library}

Briefly, APOC (Awesome Procedures on Cypher) provides, out of the box (by
placing a jar file into a database's plugins directory), many convenient,
utility functions to use. Of note are:

\begin{itemize}

  \item the ability to examine a \emph{meta-graph} showing which labels and
    relationship-types are available in the database and how they are connected, 
    allowing a user to see an overview of the model;

  \item a few key graph algorithms, such as node and path expansion, spanning
    tree, Dijkstra's shortest paths, A*, label propagation (for community
    detection), centrality measures (betweenness and closeness) and PageRank;

  \item some utility functions for regular-expressions and mathematics, useful
    for selecting nodes based on statistics (e.g. PageRank) or selecting nodes
    based on their path.

\end{itemize}

Each of these can be called directly from within Cypher; for example, writing
\texttt{call apoc.meta.graph()} would return the meta-graph of the database.

% \subsubsection{Additions}
%
% APOC is used as a foundation for simpler, domain-specific query library.  As an
% example, a user can write \texttt{coq.assumptions(<node>, <depth>)} to examine
% the n\textsuperscript{th} level assumptions behind a given node. More
% convenience functions are defined for executing some of the listed algorithms on
% all nodes in a given module or of a given type. For more complex, efficient and
% scalable/scriptable analyses however, igraph and R must be used.

\subsection{R Library}

For the R side of the library of queries, to meet requirement~\ref{req:c1} for
a core set of good, out-of-the-box defaults, I wrote several example programs
written which automatically processed data from a new database, stored the
information again for later use (to avoid re-computation) and output
appropriate visualisations. Since processing could take on the order of
minutes, I also provided status updates, informing the user of the task being
executed, the time it took to execute once complete, as well as progress-bars
where possible and relevant (e.g.\ committing a transaction to the database).

\subsubsection{Visualisation}

There are many interesting ways to visualise the plethora of data that
analysing large mathematical theories in the form of Coq libraries using graph
databases makes available.

Simply plotting the density/spread of and correlation between metrics such as
in- and out-degrees, PageRank, centrality measures is a good start and can
occasionally yield useful information. Here, R's strength as a
statistics-oriented programming language shone through, making it easy for me
to rapidly explore different ideas.

Another, more direct, way of displaying information is by displaying the graph
itself. As I will show in
Section~\ref{sec:libeval}~\nameref{sec:libeval},~\nameref{sec:libeval}, there
are a number of different layouts possible (layered, circular and
force-directed), each contributing to a more comprehensive, overall, picture of
understanding the theory.

Surprisingly, igraph was able to help with this aspect of the project as well.
Whilst visNetwork -- with its own JavaScript, force-directed, physics rendering
-- produced more aesthetically pleasing results for smaller graphs, the webpages
it output for larger graphs took intolerably long to render inside a typical
web-browser (Firefox/Opera). Thanks to an (experimental) integration with igraph
(specifically, igraph's layout mechanisms), I could pre-compute graph layouts in
fast, native C/C++ \emph{before} rendering graphs in a web-browser.

\section{Project Related}

During implementation, I learn several skills and lessons about correct project
management. Small things, such as grep-ing a code base or keeping track of time
and a log of work done, proved to be useful. However, to ensure the project ran
smoothly on a larger-scale, I focussed more on the following areas.

\subsection{Testing}

For most of the project, I conducted testing manually inspecting output. Though
this was tedious, it was the only way to do so when the model was undergoing
continual development. As soon as the model was settled and focus shifted to
visualisation, I used automated tests (which came in particularly useful when
the project had to be \emph{bifurcated} to support two different versions of
Coq).

I used some of my older Coq proof-scripts -- of problems I solved when learning
Coq -- to check output on a small scale (where I knew every single aspect of the
library, and testing turnaround was quick). I used Coq's Standard Library as a
large-scale stress-test, to ensure all constructs were translated correctly.
When I did find problems, they could usually be traced back to output from debug
statements I had embedded into ``dpdgraph++''.

\subsection{Continuous-Integration Builds}

Although the project never failed to build locally, there were occasionally
problems when trying to build it on the project supervisor's machine. The
problem was compounded when it became apparent that smaller libraries (such as
CoqRegExp and the solved problems) relied on a version of Coq (8.5.2) older than
the one Mathematical Components (needed for the project's moonshot, the Odd
Order Theorem) relied on (8.6). I set up Travis-CI for continuous-integration
builds to make version dependencies precise and explicit, and removed much of
the hassle and uncertainty surrounding builds on other machines.

\subsection{Tooling}

At first, I ran the project half on Windows and half on a Linux VM (virtual
machine), using shared folders. I had a number of reasons for this: I had
already set up Coq and Neo4j on Windows, both are easier to interact with in a
graphical environment and starting up a VM just for some experimentation took
too long.

Eventually, this set-up became confusing and time-consuming and I made the leap
to running the project fully on a Linux VM. Nevertheless, I had serious issues
when working on R integrations: running a Java database inside a Linux VM was
insufferably slow and switching databases was not easy. I solved this problem by
setting up SSH reverse-port-forwarding (to let R inside the Linux VM connect to
a Neo4j instance running directly on Windows) for decent performance.

To be the most productive during longer sessions of work, I also set up editor
integrations for OCaml, dramatically reducing the edit-compile cycle (especially
for a strong, statically-typed programming language as OCaml). At this point,
Coq's use of non-standard OCaml features, extensions and build-systems became
particularly frustrating. For example, I spent a considerable time was spent
untangling the Makefile inherited from dpdgraph to have cleaner, out-of-source
builds and reduce the mess in the current working directory, all to no avail as
I realised how complex of the build-system for Coq plugins is.

\section{Dead-ends}

I have now finished describing the whole project; all parts of
Figure~\ref{fig:structure}. I will now explain why I did not further pursue a
couple of seemingly obvious, alternative strategies to modelling Coq libraries.
Both centre around the same theme: analysing the Coq source-files directly
instead of compiling them and then analysing the \texttt{.vo} files.


\subsection{Coqdoc}

I tried to modify coqdoc's output into a useful format but this did not prove
fruitful because the purely lexical tokenisation coqdoc does cannot infer or
preserve as much information as full parsing.

\subsection{Coq source-files to CSV}

So, why not try parsing the source-files directly? This is an appealing idea,
because with my current approach, there is still some information being lost
during compilation: there is an \emph{apparent} absence or duplication of some
modules and a lack of notation and tactics.

The former arises from the use of \emph{functors}: modules that take other
modules as arguments (used to abstract over arguments, tactics, definitions and
proofs). A concrete example (from the Coq Standard Library) is the theory of
total orders, minimums and maximums, which is applied to naturals, integers and
rational numbers (as well as any further ordered types). Such functors cannot be
compiled unless fully applied (explaining the \emph{absence} of some modules);
when fully applied, they essentially copy their \emph{structure} into each
instance (explaining the \emph{duplication} of some module names and
structures).

\subsubsection{Exploring Solutions}

However, \emph{several} issues stop parsing from being a pragmatic solution,
least of which are the size of the AST and complexity of parsing Coq files (Coq
uses an \emph{extensible grammar} by using non-standard OCaml tools, making the
whole situation quite ugly to work with).

\begin{enumerate}

  \item \emph{Modules and functors are represented identically} in the AST, with
    the former as a special case of the latter, making it very difficult to
    distinguish between the two on an AST-level. By far, this was the biggest
    roadblock.

  \item \emph{Module types}, or \emph{signatures}, must be incorporated into
    the model for functors to make sense. Modules and signatures share a
    many-to-many relationship: a signature can be satisfied by multiple modules
    and a module can satisfy multiple signatures. To express this correctly
    would require \emph{signature-matching}, a notoriously difficult task (and
    the reason why few languages support ML-style modules).

  \item Further issues involve resolving objects into a global namespace and
    knowing which compiler flags were given during compilation (to match
    physical directories to logical modules), all of which complicate matching
    and merging with the compiled proof objects.

\end{enumerate}

Even though I had made some progress in tackling these issues via parsing, I
kept looking for a simpler solution. \emph{Glob files}, produced during
compilation, retain much of the information in the source code (as a list of
globally-resolved names and paths for each file) in a simpler format. I
prototyped a tool (in the form of a shell-script) that converted glob files to
CSVs. Although it was promising for tactics and notation, glob files suffer from
the same inability to distinguish between modules and functors, so I abandoned
it.

\subsubsection{Resolution}

Both direct-parsing and glob-file approaches to notation, tactics and functors
were limited and time-consuming. Functors are rarely-used with many projects
(especially given the popularity and ease of use of \emph{type-classes} for
expressing generalisations) and thus risked violation requirement~\ref{req:m1}
(by (a) potentially obfuscating information through their inclusion in the model
and (b) making the model more difficult to use).

Also, in light of requirement~\ref{req:m3}, given that names \emph{and
structures} of instantiated modules are duplicated, it could be possible to
\emph{reconstruct} generalisations \emph{representing functors} once the
database is created. As such, extracting information directly from Coq
source-files contributed little to the overall project, but presents clear ways
forward for future-work.

\section{Summary}

I just presented an in-depth account of the programs written (dpdgraph++, dpd2,
Query \& R libraries), problems encountered (recursive modules, relating types
and constructors, incorporating type signatures, CSV translation, dependency and
version tracking, impenetrable build-systems and parsing information directly
from Coq source-files), solutions implemented and tests conducted. Throughout, I
referenced the project requirements to justify important decisions.
